[1mdiff --git a/model.py b/model.py[m
[1mindex ddcb4d0..0f0e8e8 100644[m
[1m--- a/model.py[m
[1m+++ b/model.py[m
[36m@@ -147,6 +147,47 @@[m [mclass Encoder(nn.Module):[m
         return self.norm(x)[m
 [m
 class DecoderBlock(nn.Module):[m
[32m+[m[41m    [m
[32m+[m[32m    def __init__(self, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:[m
[32m+[m[32m        super().__init__()[m
[32m+[m[32m        self.self_attention_block = self_attention_block[m
[32m+[m[32m        self.cross_attention_block = cross_attention_block[m
[32m+[m[32m        self.feed_forward_block = feed_forward_block[m
[32m+[m[32m        self.residual_connections = nn.Module({ResidualConnection:(dropout) for _ in range(3)})[m[41m [m
[32m+[m
[32m+[m
[32m+[m[32m    def forward(self, x, encoder_output, src_mask, tgt_mask):[m
[32m+[m[32m        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))[m
[32m+[m[32m        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))[m
[32m+[m[32m        x = self.residual_connections[2](x, self.feed_forward_block)[m
[32m+[m[32m        return x[m[41m [m
[32m+[m[41m    [m
[32m+[m[32mclass Decoder(nn.Module):[m
[32m+[m
[32m+[m[32m    def __init__(self,layers: nn.ModuleList) -> None:[m
[32m+[m[32m        super.__init()[m
[32m+[m[32m        self.layers = layers[m
[32m+[m[32m        self.norm = LayerNormalization()[m
[32m+[m
[32m+[m[32m    def forward(self, encoder_output, x, src_mask, tgt_mask):[m
[32m+[m[32m        for layer in self.layers:[m
[32m+[m[32m            X = layer(x,encoder_output,x,src_mask,tgt_mask)[m
[32m+[m[32m        return self.norm(x)[m
[32m+[m[41m    [m
[32m+[m[41m    [m
[32m+[m[32mclass ProjectionLayer(nn.Module):[m
[32m+[m
[32m+[m[32m    def __init__(self,d_model:int, vocab_size: int) -> None:[m
[32m+[m[32m        super().__init()[m
[32m+[m[32m        self.proj = nn.Linear(d_model, vocab_size)[m
[32m+[m
[32m+[m[32m    def forward(self, x):[m
[32m+[m[32m        return torch.log_softmax (self.proj(x), dim = -1)[m
[41m+    [m
[41m+[m
[41m+[m
[41m+[m
[41m+[m
 [m
     [m
 [m
